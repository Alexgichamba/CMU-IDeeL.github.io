{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IDL 11785 Fall 2024 Lab 12 :  AutoEncoders (AEs) and Variational AutoEncoders (VAEs)\n",
        "\n",
        "## Presented By: Gabrial Zencha & Puru Samal\n",
        "#### Picture Credits: Dareen Alharthi, Harshith Kumar, Yuzhou Wang"
      ],
      "metadata": {
        "id": "dVyu-qrRht82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are AutoEncoders (AEs)\n"
      ],
      "metadata": {
        "id": "KaspmU-qjTEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AutoEncoders** (AEs) are a type of neural network designed to learn efficient representations of data, typically in an unsupervised manner. Their primary goal is to compress data (encode) and then reconstruct it (decode) as accurately as possible.\n",
        "\n",
        "## Structure of AutoEncoders\n",
        "- **Encoder**: The encoder compresses the input data into a lower-dimensional representation (often called the \"latent space\" or \"latent vector\").\n",
        "- **Latent Space**: The latent space represents the compressed information and captures the essential features of the input data.\n",
        "- **Decoder**: The decoder reconstructs the original data from the latent space representation.\n",
        " ![AutoEncoder Structure](https://drive.google.com/uc?export=view&id=1uofZ-KL_BrpQqq8fqKQpnttulZ74pGPx)\n",
        "\n",
        "\n",
        "## Key Features of AutoEncoders\n",
        "- **Unsupervised Learning**: They do not require labeled data since they aim to reconstruct the input itself.\n",
        "\n",
        "- **Compression**: AEs learn to reduce the dimensionality of data, making them useful for tasks like data compression and feature extraction.\n",
        "\n",
        "- **Reconstruction Loss**: The training objective is to minimize the difference between the input and reconstructed output, typically using a loss function like Mean Squared Error (MSE).\n",
        "\n",
        "\n",
        "## Objective Formulation of AutoEncoders\n",
        "\n",
        "- **Input:** $x$\n",
        "- **Encoding:** $z=f(x)$\n",
        "- **Latent Vector:** $z$ optained from output of Encode\n",
        "- **Decoding:** $\\hat{x} = g(z)$. Reconstructs the input from latent vector\n",
        "- **Loss:** The goal is to minimize the reconstruction error, often measured by Mean Squared Error (MSE) or Binary Cross-Entropy (BCE), depending on the type of data. Example with MSE <br>\n",
        "$Loss = || x - \\hat{x} ||^2 = ||x - g(f(x))||^2$ <br>\n",
        "Where:  \n",
        "- $x$: Original input data\n",
        "- $\\hat{x}$: = $g(f(x))$ Reconstructed data from the encoded representation\n",
        "\n",
        "## Training Objective\n",
        "The objective of training an AutoEncoder is to minimize this loss function, which forces the model to learn a latent representation that captures the important features of the input data while preserving as much information as possible during the reconstruction.\n",
        "\n",
        "## Limitations of AutoEncoders\n",
        "- **Limited Control Over the Latent Space Representation**: AutoEncoders learn a latent representation in an unsupervised manner without imposing a specific structure or distribution on the latent space. As a result, the learned latent space might not have properties like smoothness or continuity, making it difficult to interpret or control.\n",
        "- **Overfitting**: Overfitting can occur if the AutoEncoder is too complex (e.g., having too many parameters or layers) relative to the training data size. If the model memorizes the data rather than generalizing patterns, it may reconstruct training data well but fail to perform effectively on unseen data.\n",
        "\n",
        "- **Limitations in Generation**: AutoEncoders are not inherently designed for generation, they can still generate new data by sampling from the learned latent space and passing it through the decoder. However, the lack of structure in the latent space makes this challenging especially for controled generation.\n",
        "\n",
        "## How to overcome these limiations ? See Variational Autoencoders"
      ],
      "metadata": {
        "id": "ZKWtOK793ea-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are Variational AutoEncoders (VAE)\n"
      ],
      "metadata": {
        "id": "T1hlaTV3rePM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variational AutoEncoders (VAEs) are an extension of AutoEncoders that introduce a probabilistic approach to the latent space. They are commonly used in generative modeling.\n",
        "\n",
        "## Structure of Variaonal AutoEncoders\n",
        "Similar to AutoEncoders described above\n",
        "![AutoEncoder Structure](https://drive.google.com/uc?export=view&id=1bRxVo_vWxNy44BQgEOLJ6fJ_bPiB4DG-)\n",
        "\n",
        "\n",
        "## Key Properties of VAEs\n",
        "- VAEs are probabilistic models to learn data distribution\n",
        "- Map inputs to a probability distribution\n",
        "- Objective is to maximize the evidence lower bound (ELBO) : NLL of Data\n",
        "- Allows learning of a structured latent space representation\n",
        "\n",
        "\n",
        "### Loss function of VAE\n",
        "\\begin{equation}\n",
        "\\mathcal{L}(\\theta, \\phi; x^{(i)}) = -D_{KL}\\left(q_\\phi(z|x^{(i)}) \\,||\\, p_\\theta(z)\\right) + \\mathbb{E}_{q_\\phi(z|x^{(i)})} \\left[ \\log p_\\theta(x^{(i)}|z) \\right]\n",
        "\\end{equation}\n",
        "where:\n",
        "\n",
        "\n",
        "- - $ \\mathcal{L}(\\theta, \\phi; x^{(i)}) $:   is the VAE loss for a single data point $x^{(i)} $.\n",
        "- - $ \\theta $: Parameter of decoder (which maps the latent space $z$ back to the original space to reconstruct  $x$).\n",
        "- - $ \\phi $ : Parameter of encoder (which maps the input $x$ to the latent space distribution $q(z|x)$.\n",
        "\n",
        "\n",
        "### First Term: KL Divergence $-D_{KL}\\left(q_\\phi(z|x^{(i)}) \\,||\\, p_\\theta(z)\\right)\n",
        "$\n",
        "\n",
        "- - This term encourages the distribution $ q_\\phi(z|x^{(i)}) $ (the encoder output) to be close to the prior $ p_\\theta(z) $, typically $ \\mathcal{N}(0, 1) $ thereby regularizing the latent space, ensuring that the encoder outputs are distributed in a way that allows sampling from the latent space.\n",
        "\n",
        "- - By minimizing this KL divergence, we ensure that the latent space representations stay close to the prior distribution, making it easier to generate new data points by sampling from this space.\n",
        "\n",
        "### Second Term: Reconstruction Loss $\\mathbb{E}_{q_\\phi(z|x^{(i)})} \\left[ \\log p_\\theta(x^{(i)}|z) \\right]\n",
        "$\n",
        "\n",
        "- - The second term is the reconstruction loss, which encourages accurate reconstruction of the input:\n",
        "\n",
        "- - This term measures the likelihood of reconstructing the input $ x^{(i)} $ given the latent variable $ z $ sampled from $ q_\\phi(z|x^{(i)}) $.\n",
        "\n",
        "- - The expectation $\\mathbb{E}_{q_\\phi(z|x^{(i)})}$ indicates that we are taking an average over multiple samples from  $q(z|x)$\n",
        "\n",
        "- - $ \\log p_\\theta(x^{(i)}|z) $ represents the log-likelihood of the reconstructed data given the latent representation, which is high when the reconstructed data is close to the original.\n",
        "\n",
        "- - This term encourages the VAE to accurately reconstruct the input data from the latent representation, ensuring that the learned representations retain enough information to reconstruct the original input.\n",
        "\n",
        "\n",
        "## Solving Issues with AEs\n",
        "- **Limited Control Over the Latent Space Representation** <br>\n",
        " **Solution?**\n",
        "- - VAEs enforce a structured latent space by assuming that the latent variables $z$  are drawn from a known prior distribution, typically a Gaussian $p(z) = \\mathcal{N}(0,I)$\n",
        "- - The encoder outputs a mean ($\\mu$) and variance ($σ^2)$ for each dimension of and the latent variable is sampled as $z ~ \\mathcal{N}(\\mu, σ^2)$\n",
        "- -  This ensures that the latent space is smooth, continuous, and interpretable, making it easier to sample new data points or interpolate between latent representations.\n",
        "\n",
        "\n",
        "\n",
        "- **Overfitting ?** <br>\n",
        " **Solution:**\n",
        "- - The KL divergence term in the VAE loss regularizes the latent space by encouraging the learned distribution $q(z|x)$ to stay close to the prior $p(z)$\n",
        "\n",
        "- - This regularization prevents overfitting by discouraging the model from over-specializing to individual training samples and instead promotes learning generalized patterns.\n",
        "- - Additionally, the probabilistic nature of VAEs (sampling from $\\mathcal{N}(\\mu, σ^2)$) introduces randomness, acting as a form of regularization and preventing the model from purely memorizing data.\n",
        "\n",
        "- **Data Generation Limiation ?**\n",
        " **Solution**\n",
        "- - VAEs are explicitly designed as generative models. By enforcing a Gaussian prior on the latent space, the latent variables $z$ are drawn from a smooth and well-defined distribution.\n",
        "\n",
        "- - New samples can be generated by simply sampling $z. ~ \\mathcal{N}(0, I) $ and passing these samples through the decoder $p(x|z)$.\n",
        "\n",
        "- - The continuity of the latent space ensures that interpolating between points in $z$-space results in meaningful and realistic outputs.\n"
      ],
      "metadata": {
        "id": "ASNf6Jt_3Wln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding AEs and VAs"
      ],
      "metadata": {
        "id": "m4QdOGDE3iY8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfLfSlsJZNiF"
      },
      "source": [
        "\n",
        "\n",
        "Credits: [facnet-pytorch](https://github.com/timesler/facenet-pytorch) and [LFW-Face dataset](https://vis-www.cs.umass.edu/lfw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl2iQGZnET4E"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42kG62Ea7N7"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0_i-Ht4sDhED"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch --quiet\n",
        "!pip install torchinfo --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b5y_699a_3U"
      },
      "source": [
        "## LFW Face Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EBTeEkcSR7KD"
      },
      "outputs": [],
      "source": [
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "!tar -xvzf lfw.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "xMA4cJo6Alfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Checkpoints"
      ],
      "metadata": {
        "id": "_PsArBkhDEJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n"
      ],
      "metadata": {
        "id": "omgu66SqAutA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_url = \"https://drive.google.com/uc?id=19DfNOXsiEfFg8nhV-h-Lf-Vda-vi8NB_\"\n",
        "ae_output = \"ae.tar.gz\"\n",
        "gdown.download(ae_url, ae_output)\n",
        "vae_url = \"https://drive.google.com/uc?id=17s2-XgCfBOFpVHa5Gzb38UweN5Joiwjy\"\n",
        "vae_output = \"vae.tar.gz\"\n",
        "gdown.download(vae_url, vae_output)"
      ],
      "metadata": {
        "id": "eI_UnZfyBZYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf /content/vae.tar.gz\n",
        "!tar xvzf /content/ae.tar.gz"
      ],
      "metadata": {
        "id": "OzXJjWVjCgXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oppr29XEdaT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade Pillow"
      ],
      "metadata": {
        "id": "t4bx_4M6D-Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoeZHQLUHEql"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchinfo import summary\n",
        "from typing import List, Tuple\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "workers = 0 if os.name == 'nt' else 4\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tLKEnzNHseO"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UIxByzZHnWW"
      },
      "outputs": [],
      "source": [
        "# Set the part to the LFW dataset you downloaded\n",
        "data_dir = '/content/lfw'\n",
        "batch_size = 1024\n",
        "\n",
        "### -------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def collate_fn(batch: List[Tuple[Image.Image, int]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Collate function for the DataLoader to process a batch of images and labels.\n",
        "\n",
        "    Args:\n",
        "        batch (List[Tuple[Image.Image, int]]): A list of tuples, where each tuple contains:\n",
        "            - A PIL Image (Image.Image) representing an image.\n",
        "            - An integer label (int) associated with the image.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]:\n",
        "            - A tensor of images of shape (batch_size, C, H, W), where C is the number of channels,\n",
        "              H is the height, and W is the width of the images. Each image is converted to a tensor\n",
        "              from its original PIL format.\n",
        "            - A tensor of labels of shape (batch_size), containing the corresponding integer labels.\n",
        "    \"\"\"\n",
        "    # Separate images and labels from the batch\n",
        "    images, labels = zip(*batch)\n",
        "\n",
        "    # Convert each PIL image to a tensor\n",
        "    images = [transforms.ToTensor()(img) for img in images]\n",
        "\n",
        "    # Stack all images into a single tensor of shape (batch_size, C, H, W)\n",
        "    images = torch.stack(images)\n",
        "\n",
        "    # Convert labels to a tensor\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "### -------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "dataset = datasets.ImageFolder('/content/lfw')\n",
        "# We add the idx_to_class attribute to the dataset to enable easy recoding of label indices to identity names later one.\n",
        "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
        "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers, batch_size=batch_size)\n",
        "\n",
        "image_size = dataset[0][0].size[0]\n",
        "print(\"Number of classes    : \", len(dataset.classes))\n",
        "print(\"No. of train images  : \", dataset.__len__())\n",
        "print(\"Type of image        : \", type(dataset[0][0]))\n",
        "print(\"Image Dim            : \", f'{image_size}x{image_size}')\n",
        "print(\"Num Batches          : \", f'{len(loader)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSGeZcQSc_km"
      },
      "source": [
        "# Plot one batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poqx2XesI2nb"
      },
      "outputs": [],
      "source": [
        "# Set up an 8x8 grid of subplots for the 64 images in the batch\n",
        "fig, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
        "num_plots = 4 * 4  # Number of images to plot in the batch\n",
        "# Loop over the data loader\n",
        "for batch in loader:\n",
        "    # Get the batch of images and labels\n",
        "    images, labels = batch\n",
        "    # Loop over each image in the batch (assuming batch size of 64)\n",
        "    for j in range(batch_size):\n",
        "        img = images[j].permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
        "        row = j // 4  # Row index for subplot\n",
        "        col = j % 4   # Column index for subplot\n",
        "        ax[row, col].imshow(img)\n",
        "        ax[row, col].axis('off')\n",
        "        if j >= num_plots - 1:\n",
        "            break  # Stop after plotting the specified number of images\n",
        "    break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0WjFk5RHTAs"
      },
      "source": [
        "# Data Preprocessing: MTCNN For Face Detection and Cropping\n",
        "- `MTCNN` (Multi-task Cascaded Convolutional Networks) is a deep learning model for face detection, facial landmark detection, and face alignment. It uses a cascaded structure of three neural networks (P-Net, R-Net, O-Net) to progressively refine face detection and predict key facial landmarks, adjusting the face to a standard pose. It is known for its high accuracy and efficiency across various face sizes and orientations.\n",
        "- We will use MTCNN to obtain bounding boxes for detected faces and then crop those faces with th `mtcnn.detect()` function. See `help(mtcnn.detect)` for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6eRZbsQHX-l"
      },
      "outputs": [],
      "source": [
        "mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "def apply_mtcnn_crop(images: torch.Tensor, target_size=(128,128)) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Function to apply MTCNN face detection and crop the images in a batch.\n",
        "\n",
        "    Parameters:\n",
        "        images (torch.Tensor): A batch of images of shape (batch_size, C, H, W).\n",
        "        target_size (tuple): The target size (H, W) to resize images to after cropping.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A batch of cropped images.\n",
        "    \"\"\"\n",
        "    # Convert tensor batch to PIL images\n",
        "    images_pil = [transforms.ToPILImage()(img) for img in images]\n",
        "\n",
        "    cropped_images = []\n",
        "\n",
        "    for img_pil in images_pil:\n",
        "        boxes, _ = mtcnn.detect(img_pil)  # Detect faces with MTCNN\n",
        "        if boxes is not None:\n",
        "            # Crop the image using the first detected bounding box\n",
        "            x1, y1, x2, y2 = boxes[0].tolist()\n",
        "            img_pil = img_pil.crop((x1, y1, x2, y2))  # Crop the image\n",
        "        # Resize the image to the target size\n",
        "        img_pil = img_pil.resize(target_size)\n",
        "        # Convert back to tensor after cropping (if cropped)\n",
        "        img_tensor = transforms.ToTensor()(img_pil)\n",
        "        cropped_images.append(img_tensor)\n",
        "\n",
        "    # Stack the cropped images into a single tensor\n",
        "    return torch.stack(cropped_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MTCNN Processed Dataset (So you can use your own images later!)"
      ],
      "metadata": {
        "id": "dlVhKoccTYs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MTCNNPreprocessedDataset(Dataset):\n",
        "    def __init__(self, original_dataset, target_size=(128, 128), transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            original_dataset (Dataset): The original dataset to use (e.g., a standard ImageFolder).\n",
        "            target_size (tuple): The target size for resized images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.original_dataset = original_dataset  # Original dataset (e.g., ImageFolder, etc.)\n",
        "        self.target_size = target_size  # The target size after cropping\n",
        "        self.transform = transform  # Optional transform to apply to the image\n",
        "\n",
        "        # Preprocess the images during initialization\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self._preprocess_images()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return preprocessed image and its label.\n",
        "        \"\"\"\n",
        "        img, label = self.images[idx], self.labels[idx]\n",
        "\n",
        "        # Apply additional transformations if provided (e.g., normalization)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def _preprocess_images(self):\n",
        "        \"\"\"\n",
        "        Apply MTCNN face detection, crop, and resize for all images in the dataset.\n",
        "        \"\"\"\n",
        "        print(\"Copping images with MTCNN: \")\n",
        "        for idx in tqdm(range(len(self.original_dataset))):\n",
        "            img, label = self.original_dataset[idx]  # Get the image and label from the original dataset\n",
        "            img = self.apply_mtcnn_crop(img, target_size=self.target_size)  # Apply MTCNN crop\n",
        "            self.images.append(img)\n",
        "            self.labels.append(label)\n",
        "\n",
        "    def apply_mtcnn_crop(self, img: Image.Image, target_size=(128, 128)):\n",
        "        \"\"\"\n",
        "        Apply MTCNN cropping and resize the image.\n",
        "\n",
        "        Args:\n",
        "            img (PIL.Image.Image): The input image.\n",
        "            target_size (tuple): The desired output size (H, W) after cropping and resizing.\n",
        "\n",
        "        Returns:\n",
        "            PIL.Image.Image: The cropped and resized image.\n",
        "        \"\"\"\n",
        "        boxes, _ = mtcnn.detect(img)  # Detect faces with MTCNN\n",
        "        if boxes is not None:\n",
        "            # Crop the image using the first detected bounding box\n",
        "            x1, y1, x2, y2 = boxes[0].tolist()\n",
        "            img = img.crop((x1, y1, x2, y2))  # Crop the image\n",
        "        # Resize the image to the target size\n",
        "        img = img.resize(target_size)\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "Zznr_EAlTVtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a MTCNN-PreProcessed dataset/dataloader"
      ],
      "metadata": {
        "id": "BiYzWXMAnhq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform: Convert to Tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create the custom MTCNN preprocessed dataset\n",
        "mtcnn_preprocessed_dataset = MTCNNPreprocessedDataset(dataset, target_size=(128, 128), transform=transform)\n",
        "\n",
        "# Create a DataLoader for the preprocessed images\n",
        "batch_size = 1024\n",
        "mtcnn_preprocessed_loader = DataLoader(mtcnn_preprocessed_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "lS-qcT7kTp0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Cropped Face Images detected by MTCNN"
      ],
      "metadata": {
        "id": "W6J9T94OnrXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of batches and shape of each batch\n",
        "print(f'Number of batches: {len(mtcnn_preprocessed_loader)}')\n",
        "\n",
        "# Set up an 8x8 grid of subplots for the 64 images in the batch\n",
        "fig, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
        "num_plots = 4 * 4  # Number of images to plot in the batch\n",
        "# Loop over the data loader\n",
        "for batch in mtcnn_preprocessed_loader:\n",
        "    # Get the batch of images and labels\n",
        "    images, labels = batch\n",
        "    print(f'  Image batch shape: {images.shape}')\n",
        "    print(f'  Label batch shape: {labels.shape}')\n",
        "    # Loop over each image in the batch\n",
        "    for j in range(batch_size):\n",
        "        img = images[j].permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
        "        row = j // 4  # Row index for subplot\n",
        "        col = j % 4   # Column index for subplot\n",
        "        ax[row, col].imshow(img)\n",
        "        ax[row, col].axis('off')\n",
        "        if j >= num_plots - 1:\n",
        "            break  # Stop after plotting the specified number of images\n",
        "    break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KL0_OOPBntFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The AutoEncoder\n",
        "\n",
        "This Autoencoder architecture uses a series of convolutional layers to compress input images into a 128-dimensional latent space and then reconstructs them back using deconvolution layers. The Binary Cross-Entropy loss helps guide the network to minimize reconstruction errors, making the output images as close as possible to the originals.\n",
        "- **Input**: Images of shape \\([batch\\_size, 3, 128, 128]\\) (3 channels, 128x128 resolution).\n",
        "- **Encoder**: Series of convolutional layers that progressively reduce the spatial dimensions, capturing higher-level features. The final output from the encoder is flattened to a 1D vector and then passed through a fully connected layer to produce the latent representation (dimensionality defined by `latent_space_size`).\n",
        "- **Latent Space**: Fully connected layer compressing the encoded features into a 512-dimensional vector (latent representation).\n",
        "- **Decoder**: Series of upscaling + convolutional layers that upsample the latent vector back to the original input shape.\n",
        "- **Output**: Reconstructed image with the same shape as the input: \\([batch\\_size, 3, 128, 128]\\).\n",
        "\n",
        "\n",
        "### Loss Function\n",
        "The reconstruction loss used is **Mean Squared Loss (MSE)**, which measures the similarity between the original input and the reconstructed output:\n",
        "\n",
        "$\\text{recon_loss} = \\text{MSE}(recon\\_x, x)$\n",
        "\n",
        "Where:\n",
        "- \\(x\\) is the original image.\n",
        "- \\(recon\\_x\\) is the reconstructed image from the decoder."
      ],
      "metadata": {
        "id": "8aoPXdYJB62f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, in_ch=3, ndf=32, latent_space_size=256):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.latent_space_size = latent_space_size\n",
        "        self.ndf = ndf\n",
        "\n",
        "        # Encoder: Convolutional layers to extract features\n",
        "        # [batch_size,     3, 128, 128]  -> [batch_size,   ndf, 64, 64]\n",
        "        # [batch_size,   ndf,  64,  64]  -> [batch_size, ndf*2, 32, 32]\n",
        "        # [batch_size, ndf*2,  32,  32]  -> [batch_size, ndf*4, 16, 16]\n",
        "        # [batch_size, ndf*4,  16,  16]  -> [batch_size, ndf*8,  8,  8]\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, ndf, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "        # Latent space size\n",
        "        # FC layer to output latent vector\n",
        "        # Fully connected to latent space\n",
        "        self.flattened_size = ndf*8*8*8\n",
        "        self.fc = nn.Linear(self.flattened_size, self.latent_space_size)\n",
        "        # Decoder: Convolutional Transpose layers (Deconvolution)\n",
        "        self.fc_decode = nn.Linear(self.latent_space_size, self.flattened_size)\n",
        "\n",
        "        # Encoder: Convolutional layers to extract features\n",
        "        # [batch_size, ndf*8,  8,  8]  -> [batch_size, ndf*4,  16,  16]\n",
        "        # [batch_size, ndf*4, 16, 16]  -> [batch_size, ndf*2,  32,  32]\n",
        "        # [batch_size, ndf*2, 32, 32]  -> [batch_size,   ndf,  64,  64]\n",
        "        # [batch_size, ndf,   64, 64]  -> [batch_size,     3, 128, 128]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf*8, ndf*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf*4, ndf*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf*2, ndf, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(ndf),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf, in_ch, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encodes the input image to the latent representation.\n",
        "        :param x: Input image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :return: Latent representation with shape [batch_size, latent_space_size]\n",
        "        \"\"\"\n",
        "        x = self.encoder(x)  # Output shape: [batch_size, ndf*8, 8, 8]\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        return self.fc(x)\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Decodes the latent space representation back into the reconstructed image.\n",
        "        :param z: Latent space tensor with shape [batch_size, latent_space_size]\n",
        "        :return: Reconstructed image tensor with shape [batch_size, 3, 128, 128]\n",
        "        \"\"\"\n",
        "        z = self.fc_decode(z)  # Output shape: [batch_size, flattened_size]\n",
        "        z = z.view(z.size(0), self.ndf*8, 8, 8)  # Reshape\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass: Encodes and decodes the input image.\n",
        "        :param x: Input image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :return: Reconstructed image tensor with shape [batch_size, 3, 128, 128]\n",
        "        \"\"\"\n",
        "        latent = self.encode(x)  # Output shape: [batch_size, latent_space_size]\n",
        "        decoded = self.decode(latent)  # Output shape: [batch_size, 3, 128, 128]\n",
        "        return decoded\n",
        "\n",
        "    def loss_function(self, recon_x: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculates the Autoencoder loss, which consists of the reconstruction loss (BCE).\n",
        "        :param recon_x: Reconstructed image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :param x: Original image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :return: Total loss value (BCE loss)\n",
        "        \"\"\"\n",
        "        # Reconstruction loss (BCE Loss w logits, sigmoid is applied)\n",
        "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "        return recon_loss\n"
      ],
      "metadata": {
        "id": "DWBJNenTB5_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the autoencoder\n",
        "AEModel = Autoencoder()\n",
        "summary(AEModel.to(device), input_data=[images.to(device)])"
      ],
      "metadata": {
        "id": "-n31cTM8DMSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Variational AutoEncoder (VAE)\n",
        "\n",
        "A **Variational Autoencoder (VAE)** is a generative model that learns the distribution of data in a latent space, allowing for the generation of new data points similar to the input data. VAEs consist of two main parts: the **Encoder** and the **Decoder**. The encoder maps input data to a distribution in the latent space, while the decoder reconstructs data from the latent space representation. A VAE introduces a probabilistic twist to the traditional autoencoder by modeling the latent space as a distribution instead of a fixed vector, and optimizing the model using both reconstruction loss and the Kullback-Leibler (KL) divergence.\n",
        "\n",
        "### 1. **Encoder**\n",
        "\n",
        "A key difference from the AE encoder is that instead of directly outputting a single point in the latent space, the encoder outputs two values:\n",
        "- **Mean (μ)** of the distribution.\n",
        "- **Log variance (log(σ²))** to define the spread of the distribution.\n",
        "\n",
        "This means that the encoder outputs parameters for a Gaussian distribution, which will be used for sampling a point in the latent space.\n",
        "\n",
        "### 2. **Latent Space Sampling (Reparameterization Trick)**\n",
        "\n",
        "Once we have the mean (μ) and log variance (log(σ²)) from the encoder, we need to sample a point from the latent space. However, direct sampling from this distribution is not feasible during backpropagation because we need to compute gradients. The **reparameterization trick** allows us to sample from the Gaussian distribution in a way that is differentiable, enabling gradient flow through the sampling process.\n",
        "\n",
        "This trick works as follows:\n",
        "- **std = exp(0.5 * log_var)**: The standard deviation is derived from the log variance.\n",
        "- **z = mu + eps * std**: We sample `eps` from a standard normal distribution (N(0,1)) and then use it to scale and shift the distribution defined by μ and σ².\n",
        "\n",
        "### 3. **Decoder**\n",
        "\n",
        "The decoder takes the sampled latent vector (`z`) and maps it back to the data space (e.g., the original image). It is identical to the AE decoder.\n",
        "\n",
        "### 4. **Loss Function**\n",
        "\n",
        "The loss function for the VAE consists of two parts:\n",
        "1. **Reconstruction Loss**: This measures how well the decoder can reconstruct the original input from the latent space.\n",
        "   - `recon_loss = F.mse_loss(recon_x, x, reduction='sum')`\n",
        "\n",
        "2. **KL Divergence**: This measures the difference between the learned distribution (the distribution defined by the encoder's outputs μ and log(σ²)) and the prior distribution, which is typically a standard normal distribution (N(0, 1)). The KL divergence term helps to regularize the latent space by pushing the learned distribution closer to a standard normal distribution.\n",
        "   - `kl_div = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())`\n",
        "\n",
        "The final VAE loss is the sum of these two components:\n",
        "- `total_loss = recon_loss + kl_weight * kl_div`\n"
      ],
      "metadata": {
        "id": "yJeFRZK_E7WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, in_ch=3, ndf=32, latent_space_size=256):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "\n",
        "        self.latent_space_size = latent_space_size\n",
        "        self.ndf = ndf\n",
        "\n",
        "        # Encoder: Convolutional layers to extract features\n",
        "        # [batch_size,     3, 128, 128]  -> [batch_size,    ndf, 64, 64]\n",
        "        # [batch_size,   ndf,  64,  64]  -> [batch_size,  ndf*2, 32, 32]\n",
        "        # [batch_size, ndf*2,  32,  32]  -> [batch_size,  ndf*4, 16, 16]\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, ndf, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "        # Latent space size (for VAE, we need mu and log_var for the latent distribution)\n",
        "        self.flattened_size = ndf*8*8*8\n",
        "\n",
        "        # Fully connected layer to output the mean and log variance for the latent space\n",
        "        self.fc_mu = nn.Linear(self.flattened_size, self.latent_space_size)  # Mean of latent space\n",
        "        self.fc_log_var = nn.Linear(self.flattened_size, self.latent_space_size)  # Log variance of latent space\n",
        "\n",
        "\n",
        "        # Decoder: Convolutional Transpose layers (Deconvolution)\n",
        "        self.fc_decode = nn.Linear(self.latent_space_size, self.flattened_size)  # Fully connected to latent space\n",
        "\n",
        "        # Encoder: Convolutional layers to extract features\n",
        "        # [batch_size,   ndf*8,  8,  8]  -> [batch_size, ndf*4,  16,  16]\n",
        "        # [batch_size,   ndf*4, 16, 16]  -> [batch_size, ndf*2,  32,  32]\n",
        "        # [batch_size,   ndf*2, 32, 32]  -> [batch_size,   ndf,  64,  64]\n",
        "        # [batch_size,     ndf, 64, 64]  -> [batch_size,     3, 128, 128]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf*8, ndf*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf*4, ndf*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf*2, ndf, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(ndf),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(ndf, in_ch, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encodes the input image to the latent distribution (mean and log variance).\n",
        "        :param x: Input image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :return: mean (mu) and log variance (log_var) tensors with shape [batch_size, latent_space_size]\n",
        "        \"\"\"\n",
        "        x = self.encoder(x)  # Output shape: [batch_size, ndf*8, 8, 8]\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        mu = self.fc_mu(x)  # Mean of latent space [batch_size, latent_space_size]\n",
        "        log_var = self.fc_log_var(x)  # Log variance of latent space [batch_size, latent_space_size]\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Reparameterization trick: Sample z from N(mu, sigma^2) using mu and log_var.\n",
        "        :param mu: Mean of latent space with shape [batch_size, latent_space_size]\n",
        "        :param log_var: Log variance of latent space with shape [batch_size, latent_space_size]\n",
        "        :return: Latent vector z sampled from N(mu, sigma^2)\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * log_var)  # Standard deviation from log_var\n",
        "        eps = torch.randn_like(std)     # Sample from N(0, 1)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Decodes the latent space representation back into the reconstructed image.\n",
        "        :param z: Latent space tensor with shape [batch_size, latent_space_size]\n",
        "        :return: Reconstructed image tensor with shape [batch_size, 3, 64, 64]\n",
        "        \"\"\"\n",
        "        z = self.fc_decode(z)  # Output shape: [batch_size, flattened_size]\n",
        "        z = z.view(z.size(0), self.ndf*8, 8, 8)  # Reshape\n",
        "        return self.decoder(z)  # Output shape: [batch_size, 3, 128, 128]\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass: Encodes the input, applies reparameterization trick, and decodes.\n",
        "        :param x: Input image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :return: Reconstructed image tensor with shape [batch_size, 3, 128, 128]\n",
        "        \"\"\"\n",
        "        mu, log_var = self.encode(x)          # Output shape: [batch_size, latent_space_size]\n",
        "        z = self.reparameterize(mu, log_var)  # Sample from latent distribution\n",
        "        decoded = self.decode(z)\n",
        "        return decoded, mu, log_var\n",
        "\n",
        "    def loss_function(self, recon_x: torch.Tensor, x: torch.Tensor, mu: torch.Tensor, log_var: torch.Tensor, kl_weight:float=0.00025) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculates the VAE loss, which is the sum of the reconstruction loss and the KL divergence.\n",
        "        :param recon_x: Reconstructed image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :param x: Original image tensor with shape [batch_size, 3, 128, 128]\n",
        "        :param mu: Mean of the latent space with shape [batch_size, latent_space_size]\n",
        "        :param log_var: Log variance of the latent space with shape [batch_size, latent_space_size]\n",
        "        :param kl_weight: Weight for the KL divergence term\n",
        "        :return: Total VAE loss value\n",
        "        \"\"\"\n",
        "        # Reconstruction loss (BCE Loss)\n",
        "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "\n",
        "        # KL divergence between the learned distribution and the standard normal\n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp())\n",
        "\n",
        "        # Total VAE loss is the sum of reconstruction loss and KL divergence\n",
        "        return recon_loss + kl_weight * kl_div\n"
      ],
      "metadata": {
        "id": "ZxwpnLNfE_KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the autoencoder\n",
        "VAEModel = VariationalAutoencoder()\n",
        "summary(VAEModel.to(device), input_data=[images.to(device)])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gg0eCQQ1Foy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "9VycvWxdI5r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class KLWeightScheduler:\n",
        "    def __init__(self, kl_weight_max: float, total_epochs: int, low_epochs: int = 5, warmup_epochs: int = 10):\n",
        "        \"\"\"\n",
        "        S-shaped growth scheduler for KL weight with an initial low period.\n",
        "\n",
        "        :param kl_weight_max: The maximum KL weight at the end of warm-up.\n",
        "        :param total_epochs: The total number of epochs for training.\n",
        "        :param low_epochs: The number of epochs the KL weight remains low.\n",
        "        :param warmup_epochs: The number of epochs over which the KL weight increases.\n",
        "        \"\"\"\n",
        "        self.kl_weight_max = kl_weight_max\n",
        "        self.total_epochs = total_epochs\n",
        "        self.low_epochs = low_epochs\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.last_epoch = 0\n",
        "\n",
        "    def get_kl_weight(self):\n",
        "        \"\"\"\n",
        "        Calculate the KL weight using a modified sigmoid growth function.\n",
        "        :return: The current KL weight value.\n",
        "        \"\"\"\n",
        "        # Before the warm-up phase, keep the weight low\n",
        "        if self.last_epoch < self.low_epochs:\n",
        "            kl_weight = 0.0\n",
        "        else:\n",
        "            # Calculate the progress in the warm-up phase\n",
        "            progress = (self.last_epoch - self.low_epochs) / self.warmup_epochs\n",
        "            # Sigmoid function to generate S-shaped curve\n",
        "            kl_weight = self.kl_weight_max / (1 + math.exp(-10 * (progress - 0.5)))\n",
        "\n",
        "            # Ensure kl_weight does not exceed kl_weight_max\n",
        "            kl_weight = min(kl_weight, self.kl_weight_max)\n",
        "\n",
        "        return kl_weight\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Increment the epoch count for KL weight scheduling.\n",
        "        \"\"\"\n",
        "        self.last_epoch += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_step(model: nn.Module, optimizer, train_loader: DataLoader, kl_weight:float=0.0):\n",
        "    \"\"\"\n",
        "    Perform a single training step over the entire training dataset with exponential warm-up for KL weight.\n",
        "    :param model: The neural network model to be trained.\n",
        "    :param optimizer: The optimizer used to update the model's parameters.\n",
        "    :param train_loader: DataLoader for the training dataset, providing batches of data.\n",
        "    :param kl_weight: The KL weight for the model.\n",
        "    :return: The average loss over the training dataset.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "\n",
        "    # Wrap the train_loader with tqdm to show the progress bar\n",
        "    with tqdm(train_loader, unit=\"batch\", desc=f\"[Training]:\") as pbar:\n",
        "        for data, _ in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move data to GPU if available\n",
        "            data = data.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            if isinstance(model, VariationalAutoencoder):\n",
        "                reconstructed, mu, log_var = model(data)\n",
        "                loss = model.loss_function(reconstructed, data, mu, log_var, kl_weight=kl_weight)\n",
        "            else:\n",
        "                reconstructed = model(data)\n",
        "                loss = model.loss_function(reconstructed, data)\n",
        "\n",
        "            # Backward pass with scaled gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate total loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update the progress bar with the current average batch loss\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "            # Clean up\n",
        "            del data, reconstructed, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate the average loss for the epoch\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def validate_step(model: nn.Module, val_loader: DataLoader, epoch: int, path:str):\n",
        "  \"\"\"\n",
        "  Runs a validation step and plots a few images with their reconstructions.\n",
        "  Saves the plot as an image file.\n",
        "\n",
        "  :param model: Trained model.\n",
        "  :param val_loader: DataLoader for the validation dataset.\n",
        "  :param epoch: Current epoch number.\n",
        "  :path (str): File path for saving.\n",
        "  \"\"\"\n",
        "  root = os.path.join(path, 'validation_images')\n",
        "  os.makedirs(root, exist_ok=True)\n",
        "\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  with torch.no_grad():\n",
        "      # Get a batch of validation data\n",
        "      data, _ = next(iter(val_loader))\n",
        "      data = data.cuda()  # Move data to GPU if available\n",
        "\n",
        "      # Forward pass to get reconstructions\n",
        "      if isinstance(model, VariationalAutoencoder):\n",
        "          reconstructed, _, _ = model(data)\n",
        "      else:\n",
        "          reconstructed = model(data)\n",
        "\n",
        "      # Move data back to CPU for plotting\n",
        "      data = data.cpu()\n",
        "      reconstructed = reconstructed.cpu()\n",
        "\n",
        "      # Plot original and reconstructed images\n",
        "      fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "      for i in range(5):\n",
        "          # Original images\n",
        "          axes[0, i].imshow(data[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "          axes[0, i].set_title(\"Original\")\n",
        "          axes[0, i].axis('off')\n",
        "\n",
        "          # Reconstructed images\n",
        "          axes[1, i].imshow(reconstructed[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "          axes[1, i].set_title(\"Reconstructed\")\n",
        "          axes[1, i].axis('off')\n",
        "\n",
        "      plt.tight_layout()\n",
        "      impath = os.path.join(root, f'validation_epoch_{epoch}.png')\n",
        "      plt.savefig(impath)\n",
        "      plt.close(fig)\n",
        "      print(f'Saved validation results for epoch {epoch} as validation_epoch_{epoch}.png')\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    \"\"\"\n",
        "    Saves the model, optimizer, and scheduler states along with a metric to a specified path.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model to be saved.\n",
        "        optimizer (Optimizer): Optimizer state to save.\n",
        "        scheduler (Scheduler or None): Scheduler state to save.\n",
        "        metric (tuple): Metric tuple (name, value) to be saved.\n",
        "        epoch (int): Current epoch number.\n",
        "        path (str): File path for saving.\n",
        "    \"\"\"\n",
        "    # Ensure metric is provided as a tuple with correct structure\n",
        "    if not (isinstance(metric, tuple) and len(metric) == 2):\n",
        "        raise ValueError(\"metric must be a tuple in the form (name, value)\")\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler else {},\n",
        "            metric[0]: metric[1],  # Unpacks the metric name and value\n",
        "            \"epoch\": epoch\n",
        "        },\n",
        "        path\n",
        "    )\n",
        "\n",
        "\n",
        "def load_model(model, optimizer, scheduler, path):\n",
        "    \"\"\"\n",
        "    Loads the model, optimizer, and scheduler states along with a saved metric and epoch from a specified path.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model instance to load the state into.\n",
        "        optimizer (Optimizer or nNone): Optimizer instance to load the state into.\n",
        "        scheduler (Scheduler or None): Scheduler instance to load the state into, if applicable.\n",
        "        path (str): File path to load the checkpoint from.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the metric (name, value) and the last saved epoch.\n",
        "    \"\"\"\n",
        "    # Load the checkpoint from the specified path\n",
        "    checkpoint = torch.load(path)\n",
        "\n",
        "    # Load the model's state dictionary\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Load the optimizer's state dictionary\n",
        "    if optimizer:\n",
        "      optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    # Load the scheduler's state dictionary, if provided\n",
        "    if scheduler:\n",
        "        scheduler_state = checkpoint.get(\"scheduler_state_dict\", {})\n",
        "        if scheduler_state:\n",
        "            scheduler.load_state_dict(scheduler_state)\n",
        "\n",
        "    # Retrieve the metric from the checkpoint\n",
        "    # Identify the metric key (excluding reserved keys)\n",
        "    metric_keys = [key for key in checkpoint.keys() if key not in {\"model_state_dict\", \"optimizer_state_dict\", \"scheduler_state_dict\", \"epoch\"}]\n",
        "    if len(metric_keys) != 1:\n",
        "        raise ValueError(\"Unexpected format: More than one metric key found in checkpoint.\")\n",
        "    metric_name = metric_keys[0]\n",
        "    metric_value = checkpoint[metric_name]\n",
        "\n",
        "    # Retrieve the last saved epoch\n",
        "    epoch = checkpoint.get(\"epoch\", 0)\n",
        "\n",
        "    return (metric_name, metric_value), epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "q8sMz28_I5EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Autoencoder\n",
        "A `ae_checkpoints` directory will have been created where the model checkpoints are saved. Additionally, every 10 epochs, 5 samples are reconstructed and compared to the original. The images are saved in `ae_checkpoints/validation_imgs`. You can use this to monitor the performance of your Autoencoder."
      ],
      "metadata": {
        "id": "rg7XA2aws8rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "checkpoint_root = os.path.join(os.getcwd(), 'ae_checkpoints')\n",
        "os.makedirs(checkpoint_root, exist_ok=True)\n",
        "checkpoint_filename = 'ae_model'\n",
        "\n",
        "# choose model to train\n",
        "model = AEModel\n",
        "\n",
        "# set your epochs for this approach\n",
        "# Set up the optimizer\n",
        "# Set up the scheduler\n",
        "epochs = 100\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1E-8)\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    train_loss = train_step(model, optimizer, mtcnn_preprocessed_loader)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Learning Rate {:.06f}\".format(\n",
        "        epoch + 1, epochs, train_loss, curr_lr))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        validate_step(model, mtcnn_preprocessed_loader, epoch+1, checkpoint_root)\n",
        "\n",
        "    if best_loss >= train_loss:\n",
        "        best_loss = train_loss\n",
        "        epoch_model_path = os.path.join(checkpoint_root, (checkpoint_filename + str(epoch) + '.pth'))\n",
        "        save_model(model, optimizer, scheduler, ('loss', train_loss), epoch, epoch_model_path)\n",
        "        print(\"Saved best loss model\")\n",
        "#### ----------------------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "fN1yZeYgWjSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDQGtkA1-SxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train VAE\n",
        "A `vae_checkpoints` directory will have been created where the model checkpoints are saved. Additionally, every 10 epochs, 5 samples are reconstructed and compared to the original. The images are saved in `vae_checkpoints/validation_imgs`. You can use this to monitor the performance of your Autoencoder."
      ],
      "metadata": {
        "id": "WtcaMv4ttdWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "checkpoint_root = os.path.join(os.getcwd(), 'vae_checkpoints')\n",
        "os.makedirs(checkpoint_root, exist_ok=True)\n",
        "checkpoint_filename = 'vae_model'\n",
        "\n",
        "# choose model to train\n",
        "model = VAEModel\n",
        "\n",
        "# set your epochs for this approach\n",
        "# Set up the optimizer\n",
        "# Set up the scheduler\n",
        "epochs = 120\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1E-8)\n",
        "\n",
        "# Define KL Weight Scheduler with linear warm-up\n",
        "kl_weight_max = 2.0  # Maximum KL weight\n",
        "kl_scheduler = KLWeightScheduler(kl_weight_max, epochs, low_epochs=10, warmup_epochs=10)\n",
        "\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
        "    kl_weight = kl_scheduler.get_kl_weight()\n",
        "\n",
        "    train_loss = train_step(model, optimizer, mtcnn_preprocessed_loader, kl_weight)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Learning Rate {:.06f}\\t KL Weight {:.06f}\".format(\n",
        "        epoch + 1, epochs, train_loss, curr_lr, kl_weight))\n",
        "\n",
        "    scheduler.step()\n",
        "    kl_scheduler.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        validate_step(model, mtcnn_preprocessed_loader, epoch+1, checkpoint_root)\n",
        "\n",
        "    if best_loss >= train_loss:\n",
        "        best_loss = train_loss\n",
        "        epoch_model_path = os.path.join(checkpoint_root, (checkpoint_filename + str(epoch) + '.pth'))\n",
        "        save_model(model, optimizer, scheduler, ('loss', train_loss), epoch, epoch_model_path)\n",
        "        print(\"Saved best loss model\")\n",
        "#### ----------------------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jj3E7QMKtny5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "HwRpViMpL3H0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load checkpoints"
      ],
      "metadata": {
        "id": "8pm0Pyt_EncK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AEModel.load_state_dict(torch.load('/content/ae_checkpoints/final_ae_model99.pth')['model_state_dict'])\n",
        "VAEModel.load_state_dict(torch.load('/content/vae_checkpoints/final_vae_model149.pth')['model_state_dict'])"
      ],
      "metadata": {
        "id": "L84S0YwXEqF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Reconstruction Comparison"
      ],
      "metadata": {
        "id": "Ph4X8WyBMNrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AEModel.eval()\n",
        "VAEModel.eval()\n",
        "with torch.no_grad():\n",
        "  # Get a batch of validation data\n",
        "  data, _ = next(iter(mtcnn_preprocessed_loader))\n",
        "  data = data.cuda()  # Move data to GPU if available\n",
        "  reconstructed_ae        = AEModel(data)\n",
        "  reconstructed_vae, _, _ = VAEModel(data)\n",
        "\n",
        "  # Move data back to CPU for plotting\n",
        "  data = data.cpu()\n",
        "  reconstructed_ae = reconstructed_ae.cpu()\n",
        "  reconstructed_vae = reconstructed_vae.cpu()\n",
        "\n",
        "  # Plot original and reconstructed images\n",
        "  fig, axes = plt.subplots(3, 5, figsize=(15, 6))\n",
        "  for i in range(5):\n",
        "      # Original images\n",
        "      axes[0, i].imshow(data[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "      axes[0, i].set_title(\"Original\")\n",
        "      axes[0, i].axis('off')\n",
        "\n",
        "      # AE Reconstructed images\n",
        "      axes[1, i].imshow(reconstructed_ae[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "      axes[1, i].set_title(\"Reconstructed AE\")\n",
        "      axes[1, i].axis('off')\n",
        "\n",
        "      # VAE Reconstructed images\n",
        "      axes[2, i].imshow(reconstructed_vae[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "      axes[2, i].set_title(\"Reconstructed AE\")\n",
        "      axes[2, i].axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "iSaU8eVAi-CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling w/ random noise"
      ],
      "metadata": {
        "id": "foRY4z38M8ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sample_images(model: nn.Module, num_samples: int = 10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Sample from the latent space for VAE or directly for Autoencoder\n",
        "        z = torch.randn(num_samples, model.latent_space_size).cuda()  # Latent vector of size latent_space_size\n",
        "        if isinstance(model, VariationalAutoencoder):\n",
        "            # For VAE, we can sample from the latent space (e.g., from a normal distribution)\n",
        "            sampled_images = model.decode(z)  # Decoded images from latent space\n",
        "        else:\n",
        "            # For Autoencoder, use random input or encoded images for reconstruction\n",
        "            sampled_images = model.decode(z)  # Reconstructed images\n",
        "\n",
        "        # Plot the samples\n",
        "        sampled_images = sampled_images.cpu()\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(num_samples):\n",
        "            plt.subplot(1, num_samples, i+1)\n",
        "            plt.imshow(sampled_images[i].permute(1, 2, 0).numpy())\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VktwxGD_M-Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_images(AEModel, num_samples=5)"
      ],
      "metadata": {
        "id": "C_XJ67BsZR2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_images(VAEModel, num_samples=5)"
      ],
      "metadata": {
        "id": "nuVJ6BQGkWEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Space Exploration"
      ],
      "metadata": {
        "id": "vcbrBSGrMPC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def visualize_latent_space(autoencoder, vae, dataloader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Visualize the latent space learned by Autoencoder and Variational Autoencoder using PCA in 2D.\n",
        "\n",
        "    Parameters:\n",
        "    - autoencoder: The trained Autoencoder model.\n",
        "    - vae: The trained Variational Autoencoder model.\n",
        "    - dataloader: DataLoader object providing the dataset (e.g., MNIST).\n",
        "    - device: Device to run the models on, either 'cuda' or 'cpu'.\n",
        "    \"\"\"\n",
        "    autoencoder.eval()\n",
        "    vae.eval()\n",
        "\n",
        "    # Collect the latent representations and labels\n",
        "    ae_latents = []\n",
        "    vae_latents = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, lbls) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Get latent representations from AE and VAE\n",
        "            ae_latent = autoencoder.encode(images)\n",
        "            mu, var = vae.encode(images)\n",
        "            vae_latent = vae.reparameterize(mu, var)\n",
        "\n",
        "            ae_latents.append(ae_latent.view(ae_latent.size(0), -1))\n",
        "            vae_latents.append(vae_latent.view(vae_latent.size(0), -1))\n",
        "            labels.extend(lbls.cpu().numpy())\n",
        "\n",
        "            if i > 1:  # Only collect data from a couple of batches for visualization\n",
        "                break\n",
        "\n",
        "    # Flatten the latent representations and convert labels to numpy array\n",
        "    ae_latents = torch.cat(ae_latents).cpu().numpy()\n",
        "    vae_latents = torch.cat(vae_latents).cpu().numpy()\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Apply PCA for dimensionality reduction to 2D\n",
        "    pca = PCA(n_components=2)\n",
        "    ae_latents_2d = pca.fit_transform(ae_latents)\n",
        "    vae_latents_2d = pca.fit_transform(vae_latents)\n",
        "\n",
        "    # Plot the 2D scatter plots\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot for Autoencoder\n",
        "    scatter_ae = axes[0].scatter(ae_latents_2d[:, 0], ae_latents_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
        "    axes[0].set_title('Autoencoder Latent Space')\n",
        "    axes[0].set_xlabel('Component 1')\n",
        "    axes[0].set_ylabel('Component 2')\n",
        "    legend1 = axes[0].legend(*scatter_ae.legend_elements(), title=\"Labels\")\n",
        "    axes[0].add_artist(legend1)\n",
        "\n",
        "    # Plot for Variational Autoencoder\n",
        "    scatter_vae = axes[1].scatter(vae_latents_2d[:, 0], vae_latents_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
        "    axes[1].set_title('Variational Autoencoder Latent Space')\n",
        "    axes[1].set_xlabel('Component 1')\n",
        "    axes[1].set_ylabel('Component 2')\n",
        "    legend2 = axes[1].legend(*scatter_vae.legend_elements(), title=\"Labels\")\n",
        "    axes[1].add_artist(legend2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "k2zn392qMR3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_latent_space(AEModel, VAEModel, mtcnn_preprocessed_loader)"
      ],
      "metadata": {
        "id": "v9oZrwIKke6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Space Interpolation"
      ],
      "metadata": {
        "id": "Zp69uJxi3VzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_latent_space(model: nn.Module, start_image, end_image, num_steps=32, device='cuda'):\n",
        "    \"\"\"\n",
        "    Interpolates between two images in the latent space of the model and plots the result.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained Autoencoder or Variational Autoencoder model.\n",
        "    - start_image: The starting image for interpolation (should be in the range [0, 1] and shape (C, H, W)).\n",
        "    - end_image: The ending image for interpolation (same shape as start_image).\n",
        "    - num_steps: The number of interpolation steps.\n",
        "    - device: Device to run the model on, either 'cuda' or 'cpu'.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    start_image = start_image.unsqueeze(0).to(device)\n",
        "    end_image = end_image.unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode both images into their latent representations\n",
        "    with torch.no_grad():\n",
        "        if isinstance(model, VariationalAutoencoder):\n",
        "            # For VAE, get mu and log_var, and reparameterize z\n",
        "            mu_start, log_var_start = model.encode(start_image)\n",
        "            mu_end, log_var_end = model.encode(end_image)\n",
        "            # Reparameterize to get z (latent vectors)\n",
        "            start_latent = model.reparameterize(mu_start, log_var_start)\n",
        "            end_latent = model.reparameterize(mu_end, log_var_end)\n",
        "        else:\n",
        "            # For AE, directly use the encoded latent vector\n",
        "            start_latent = model.encode(start_image)\n",
        "            end_latent = model.encode(end_image)\n",
        "\n",
        "        # Ensure latent vectors are of the same size\n",
        "        start_latent = start_latent.view(start_latent.size(0), -1)\n",
        "        end_latent = end_latent.view(end_latent.size(0), -1)\n",
        "\n",
        "    # Create linear interpolation between the two latent representations\n",
        "    latents = []\n",
        "    for alpha in np.linspace(0, 1, num_steps):\n",
        "        interpolated_latent = (1 - alpha) * start_latent + alpha * end_latent\n",
        "        latents.append(interpolated_latent)\n",
        "\n",
        "    # Decode the interpolated latents\n",
        "    latents = torch.stack(latents).to(device)\n",
        "    if isinstance(model, VariationalAutoencoder):\n",
        "        # For VAE, we decode the reparameterized latents\n",
        "        interpolated_images = model.decode(latents)\n",
        "    else:\n",
        "        # For AE, directly decode the latents\n",
        "        interpolated_images = model.decode(latents)\n",
        "\n",
        "    # Plot the results\n",
        "    interpolated_images = interpolated_images.cpu()\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    for i, img in enumerate(interpolated_images):\n",
        "        plt.subplot(8, 8, i+1)  # 4 rows, 8 columns\n",
        "        plt.imshow(img.permute(1, 2, 0).detach().numpy())\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "21T-TW-p3Nwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_index1 = torch.randint(0, images.size(0), (1,)).item()\n",
        "random_index2 = torch.randint(0, images.size(0), (1,)).item()\n",
        "image1 = images[random_index1]\n",
        "image2 = images[random_index2]\n"
      ],
      "metadata": {
        "id": "iHP0KiOp3Ynh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate_latent_space(VAEModel, image1, image2, num_steps=64, device='cuda')"
      ],
      "metadata": {
        "id": "Radj2rzx37f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate_latent_space(AEModel, image1, image2, num_steps=64, device='cuda')"
      ],
      "metadata": {
        "id": "CoBlCl7B3-EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CH1EfxNWG1wY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}